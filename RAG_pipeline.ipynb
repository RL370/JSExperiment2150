{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPv7ElSuGbGJwNK5T2brXs8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1711e5684ff44de2b87388f013fa7d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8900e7ed418e464696c741962e90158c",
              "IPY_MODEL_b98ee10213514b9990ff7e6c01cd38e0",
              "IPY_MODEL_4f1ce9f285144946b5f1fb1ac12a5043"
            ],
            "layout": "IPY_MODEL_89ae1debdff54c16aabe7e136268d930"
          }
        },
        "8900e7ed418e464696c741962e90158c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af4511a066854c7f9ceffe3a4a68d62a",
            "placeholder": "​",
            "style": "IPY_MODEL_0581bd336ff3403e88c3ae49df6c7057",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b98ee10213514b9990ff7e6c01cd38e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3be99ee95ee84679809d27e350b77802",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3262d0d727754ba5aa799edbdbf9be01",
            "value": 2
          }
        },
        "4f1ce9f285144946b5f1fb1ac12a5043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d26319542ce44b429e5c5dd5168d774b",
            "placeholder": "​",
            "style": "IPY_MODEL_c6e3ae5895474e7780a6b9714199ca11",
            "value": " 2/2 [00:31&lt;00:00, 13.44s/it]"
          }
        },
        "89ae1debdff54c16aabe7e136268d930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af4511a066854c7f9ceffe3a4a68d62a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0581bd336ff3403e88c3ae49df6c7057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3be99ee95ee84679809d27e350b77802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3262d0d727754ba5aa799edbdbf9be01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d26319542ce44b429e5c5dd5168d774b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e3ae5895474e7780a6b9714199ca11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b3dff4a66f349149beec060272f9e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf71dd603b8948c98ed6071d00047491",
              "IPY_MODEL_270c960007a642d291006560d48249b2",
              "IPY_MODEL_a053326b78bb4455aa389562cd2396fa"
            ],
            "layout": "IPY_MODEL_21863fb0470a400cbeaf56859bbd8c48"
          }
        },
        "bf71dd603b8948c98ed6071d00047491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90e69acfe6244661ad200e9927a26ff3",
            "placeholder": "​",
            "style": "IPY_MODEL_ca4099d336b8488986b7c3fe74295e82",
            "value": "Batches: 100%"
          }
        },
        "270c960007a642d291006560d48249b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_451cdcd9574a4897aca7574c34798758",
            "max": 336,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2eda7aac026c40858388542ae41920d3",
            "value": 336
          }
        },
        "a053326b78bb4455aa389562cd2396fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_287fbe39c0d3483ba6de9f7b324bea40",
            "placeholder": "​",
            "style": "IPY_MODEL_82d60a6e4c6649e6925f0ae02c6179e1",
            "value": " 336/336 [01:43&lt;00:00, 12.14it/s]"
          }
        },
        "21863fb0470a400cbeaf56859bbd8c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e69acfe6244661ad200e9927a26ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca4099d336b8488986b7c3fe74295e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "451cdcd9574a4897aca7574c34798758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eda7aac026c40858388542ae41920d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "287fbe39c0d3483ba6de9f7b324bea40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82d60a6e4c6649e6925f0ae02c6179e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07f587ac987c4a58907ae9bd3b14cabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62a94e70ebfc43d2abac28781ef22640",
              "IPY_MODEL_5f5ca6489a214552be2e2d2a8e627d0a",
              "IPY_MODEL_ec6389b3d0f543b1948eaeb8c9fc27a4"
            ],
            "layout": "IPY_MODEL_751f8ecadc9240d4b3ac5e1001d4acc1"
          }
        },
        "62a94e70ebfc43d2abac28781ef22640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff507a9535ab4d659daaabb7448c557a",
            "placeholder": "​",
            "style": "IPY_MODEL_6311638497254375a9e95b4004739c2a",
            "value": "Computing widget examples:   0%"
          }
        },
        "5f5ca6489a214552be2e2d2a8e627d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ca19c259b0241c2ba563db79b44fc3b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c504df17719b4fc9812c747da96cd089",
            "value": 1
          }
        },
        "ec6389b3d0f543b1948eaeb8c9fc27a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa1f9fd59e2642bd9379671fce8805e7",
            "placeholder": "​",
            "style": "IPY_MODEL_c73b316b14964f15b4a94651922e4d44",
            "value": " 0/1 [00:00&lt;?, ?example/s]"
          }
        },
        "751f8ecadc9240d4b3ac5e1001d4acc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ff507a9535ab4d659daaabb7448c557a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6311638497254375a9e95b4004739c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ca19c259b0241c2ba563db79b44fc3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c504df17719b4fc9812c747da96cd089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa1f9fd59e2642bd9379671fce8805e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c73b316b14964f15b4a94651922e4d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfac69a4a9984debb872c4d776a6a970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d5f24a9b0be42f3a01e650ad40f8fe1",
              "IPY_MODEL_19bfa3bf2002429c9bc2d70b3f4c7e18",
              "IPY_MODEL_921bb8dda610483fbb916c142f1a771d"
            ],
            "layout": "IPY_MODEL_d5797639d66841e79e021b3f86f0718f"
          }
        },
        "4d5f24a9b0be42f3a01e650ad40f8fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23bf301c005c4d7bafb7407bcac22773",
            "placeholder": "​",
            "style": "IPY_MODEL_5001f9d0981d43ceb06ea44999e8acb9",
            "value": "Batches: 100%"
          }
        },
        "19bfa3bf2002429c9bc2d70b3f4c7e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88f10305fb184248bcfb4412bd0287e0",
            "max": 336,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_565c88e1a3f04aa79eb084491e870d60",
            "value": 336
          }
        },
        "921bb8dda610483fbb916c142f1a771d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b01ba22607946b2aaba906b59937fc4",
            "placeholder": "​",
            "style": "IPY_MODEL_368e133dee4c42f5834113fdce407d44",
            "value": " 336/336 [00:34&lt;00:00, 30.90it/s]"
          }
        },
        "d5797639d66841e79e021b3f86f0718f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23bf301c005c4d7bafb7407bcac22773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5001f9d0981d43ceb06ea44999e8acb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88f10305fb184248bcfb4412bd0287e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "565c88e1a3f04aa79eb084491e870d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b01ba22607946b2aaba906b59937fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "368e133dee4c42f5834113fdce407d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98ab78b6b1ac478c9931c6e317c7f615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5204afe18c314fd69d2529ad65bd8193",
              "IPY_MODEL_84a0e68a8d7847ee85118e8419b8739c",
              "IPY_MODEL_d77390ce261c4be5a6975007ba3ce1df"
            ],
            "layout": "IPY_MODEL_e9028aa06f4f47c89080798d206697a4"
          }
        },
        "5204afe18c314fd69d2529ad65bd8193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e7b0b7c1d714f2394f18576cb0a3533",
            "placeholder": "​",
            "style": "IPY_MODEL_58f2057182a24c548bb08e0c9f6151ee",
            "value": "Map: 100%"
          }
        },
        "84a0e68a8d7847ee85118e8419b8739c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_030257456ae44056b540f78040f263b0",
            "max": 1254,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf320eedbaad4a10bf7468b64849a984",
            "value": 1254
          }
        },
        "d77390ce261c4be5a6975007ba3ce1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1201e082080f4fdcbb53d5154932282a",
            "placeholder": "​",
            "style": "IPY_MODEL_e7d0119839754c278cabe28b48caa69c",
            "value": " 1254/1254 [00:00&lt;00:00, 1884.42 examples/s]"
          }
        },
        "e9028aa06f4f47c89080798d206697a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e7b0b7c1d714f2394f18576cb0a3533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58f2057182a24c548bb08e0c9f6151ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "030257456ae44056b540f78040f263b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf320eedbaad4a10bf7468b64849a984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1201e082080f4fdcbb53d5154932282a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7d0119839754c278cabe28b48caa69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RL370/JSExperiment2150/blob/main/RAG_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQLWolWqCaA7",
        "outputId": "87c18998-0b9a-4e53-8ed6-b06dae7c8010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: rag_pipeline in /usr/local/lib/python3.12/dist-packages (0.1.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from rag_pipeline) (1.108.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (from rag_pipeline) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rag_pipeline) (2.0.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (from rag_pipeline) (0.11.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu->rag_pipeline) (25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai->rag_pipeline) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai->rag_pipeline) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai->rag_pipeline) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai->rag_pipeline) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai->rag_pipeline) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai->rag_pipeline) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai->rag_pipeline) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai->rag_pipeline) (4.15.0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber->rag_pipeline) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber->rag_pipeline) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber->rag_pipeline) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber->rag_pipeline) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber->rag_pipeline) (43.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai->rag_pipeline) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai->rag_pipeline) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai->rag_pipeline) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->rag_pipeline) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai->rag_pipeline) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai->rag_pipeline) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai->rag_pipeline) (0.4.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber->rag_pipeline) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber->rag_pipeline) (2.23)\n"
          ]
        }
      ],
      "source": [
        "!# Install all required packages\n",
        "! pip install torch transformers sentence-transformers faiss-cpu rank_bm25 matplotlib seaborn bitsandbytes accelerate datasets tqdm\n",
        "! pip install rag_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Full-Scale Hybrid RAG Pipeline with Fine-tuning\n",
        "Uses complete HotpotQA dataset with training capabilities\n",
        "\"\"\"\n",
        "\n",
        "import json, re, csv, time, logging, os\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, InputExample, losses\n",
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig, TrainingArguments, Trainer, DefaultDataCollator\n",
        ")\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ======================================================================\n",
        "# Configuration\n",
        "# ======================================================================\n",
        "\n",
        "@dataclass\n",
        "class RAGConfig:\n",
        "    # Data\n",
        "    data_dir: str = \"data\"\n",
        "    output_dir: str = \"outputs\"\n",
        "    cache_dir: str = \"cache\"\n",
        "\n",
        "    # Models\n",
        "    dense_model: str = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "    rerank_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "    extractive_model: str = \"deepset/roberta-base-squad2\"\n",
        "    generative_model: str = \"microsoft/phi-2\"\n",
        "\n",
        "    # Training\n",
        "    use_full_dataset: bool = True\n",
        "    train_retriever: bool = True\n",
        "    train_extractive: bool = True\n",
        "    num_train_examples: int = -1  # -1 for all\n",
        "    num_val_examples: int = -1    # -1 for all\n",
        "\n",
        "    # Retrieval\n",
        "    retrieval_k: int = 10\n",
        "    alpha: float = 0.7  # Dense vs sparse balance\n",
        "\n",
        "    # Training hyperparameters\n",
        "    learning_rate: float = 2e-5\n",
        "    num_epochs: int = 3\n",
        "    batch_size: int = 8\n",
        "    warmup_steps: int = 500\n",
        "\n",
        "    # Hardware\n",
        "    use_gpu: bool = True\n",
        "    use_4bit_quant: bool = True\n",
        "    max_length: int = 384\n",
        "\n",
        "    # Checkpointing\n",
        "    save_checkpoints: bool = True\n",
        "    checkpoint_dir: str = \"checkpoints\"\n",
        "\n",
        "# ======================================================================\n",
        "# Data structures\n",
        "# ======================================================================\n",
        "\n",
        "@dataclass\n",
        "class Document:\n",
        "    id: str\n",
        "    title: str\n",
        "    content: str\n",
        "    sentences: List[str]\n",
        "\n",
        "@dataclass\n",
        "class RAGOutput:\n",
        "    question_id: str\n",
        "    question: str\n",
        "    answer: str\n",
        "    answer_type: str\n",
        "    gold_answer: str\n",
        "    retrieved_passages: List[Dict[str, Any]]\n",
        "    confidence_score: float\n",
        "    prompt_tokens: int\n",
        "    output_tokens: int\n",
        "    total_tokens: int\n",
        "    retrieval_scores: List[float]\n",
        "    supporting_facts: List[List]\n",
        "    retrieval_recall: float\n"
      ],
      "metadata": {
        "id": "PYcRxHyKsDir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Enhanced Hybrid Retriever with Training\n",
        "# ======================================================================\n",
        "\n",
        "class EnhancedHybridRetriever:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.use_gpu = config.use_gpu and torch.cuda.is_available()\n",
        "        device = \"cuda\" if self.use_gpu else \"cpu\"\n",
        "\n",
        "        logger.info(f\"Loading retrieval models on {device}...\")\n",
        "        self.dense_encoder = SentenceTransformer(config.dense_model, device=device)\n",
        "        self.reranker = CrossEncoder(config.rerank_model, device=device)\n",
        "\n",
        "        self.documents = []\n",
        "        self.doc_map = {}  # title -> document\n",
        "        self.tokenized_docs = []\n",
        "        self.bm25 = None\n",
        "        self.dense_index = None\n",
        "        self.document_embeddings = None\n",
        "\n",
        "    def build_index(self, documents: List[Document]):\n",
        "        \"\"\"Build search indexes from documents\"\"\"\n",
        "        logger.info(f\"Building indexes for {len(documents)} documents...\")\n",
        "        self.documents = documents\n",
        "        self.doc_map = {d.title: d for d in documents}\n",
        "\n",
        "        doc_texts = [f\"{d.title}: {d.content}\" for d in documents]\n",
        "\n",
        "        # Dense index\n",
        "        logger.info(\"Encoding documents...\")\n",
        "        self.document_embeddings = self.dense_encoder.encode(\n",
        "            doc_texts, show_progress_bar=True,\n",
        "            batch_size=32 if self.use_gpu else 16,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        self.dense_index = faiss.IndexFlatIP(self.document_embeddings.shape[1])\n",
        "        faiss.normalize_L2(self.document_embeddings)\n",
        "        self.dense_index.add(self.document_embeddings.astype(\"float32\"))\n",
        "\n",
        "        # Sparse index\n",
        "        logger.info(\"Building BM25 index...\")\n",
        "        self.tokenized_docs = [d.content.lower().split() for d in documents]\n",
        "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
        "\n",
        "        logger.info(\"✓ Indexes built successfully\")\n",
        "\n",
        "    def prepare_training_data(self, examples: List[Dict]) -> List[InputExample]:\n",
        "        \"\"\"Prepare training data for retriever fine-tuning\"\"\"\n",
        "        logger.info(\"Preparing retriever training data...\")\n",
        "        train_samples = []\n",
        "\n",
        "        for ex in tqdm(examples, desc=\"Creating training pairs\"):\n",
        "            question = ex[\"question\"]\n",
        "\n",
        "            # Positive passages (supporting facts)\n",
        "            positive_titles = set(title for title, _ in ex[\"supporting_facts\"])\n",
        "\n",
        "            for title in positive_titles:\n",
        "                if title in self.doc_map:\n",
        "                    pos_doc = self.doc_map[title]\n",
        "                    pos_text = f\"{pos_doc.title}: {pos_doc.content}\"\n",
        "                    train_samples.append(InputExample(texts=[question, pos_text], label=1.0))\n",
        "\n",
        "            # Negative passages (random non-supporting)\n",
        "            available_titles = [d.title for d in self.documents if d.title not in positive_titles]\n",
        "            if available_titles:\n",
        "                neg_titles = np.random.choice(available_titles, min(2, len(available_titles)), replace=False)\n",
        "                for title in neg_titles:\n",
        "                    if title in self.doc_map:\n",
        "                        neg_doc = self.doc_map[title]\n",
        "                        neg_text = f\"{neg_doc.title}: {neg_doc.content}\"\n",
        "                        train_samples.append(InputExample(texts=[question, neg_text], label=0.0))\n",
        "\n",
        "        logger.info(f\"Created {len(train_samples)} training pairs\")\n",
        "        return train_samples\n",
        "\n",
        "    def fine_tune(self, train_examples: List[Dict], val_examples: List[Dict]):\n",
        "        \"\"\"Fine-tune the dense retriever\"\"\"\n",
        "        logger.info(\"=\"*60)\n",
        "        logger.info(\"FINE-TUNING DENSE RETRIEVER\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        train_samples = self.prepare_training_data(train_examples)\n",
        "\n",
        "        # Create dataloader\n",
        "        train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=self.config.batch_size)\n",
        "\n",
        "        # Loss function\n",
        "        train_loss = losses.CosineSimilarityLoss(self.dense_encoder)\n",
        "\n",
        "        # Training\n",
        "        output_path = Path(self.config.checkpoint_dir) / \"dense_retriever\"\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Disable all logging/tracking\n",
        "        os.environ['WANDB_DISABLED'] = 'true'\n",
        "        os.environ['WANDB_MODE'] = 'disabled'\n",
        "\n",
        "        self.dense_encoder.fit(\n",
        "            train_objectives=[(train_dataloader, train_loss)],\n",
        "            epochs=self.config.num_epochs,\n",
        "            warmup_steps=self.config.warmup_steps,\n",
        "            output_path=str(output_path),\n",
        "            save_best_model=True,\n",
        "            show_progress_bar=True,\n",
        "            use_amp=self.use_gpu\n",
        "        )\n",
        "\n",
        "        logger.info(\"✓ Retriever fine-tuning complete\")\n",
        "\n",
        "        # Rebuild index with fine-tuned model\n",
        "        logger.info(\"Rebuilding index with fine-tuned embeddings...\")\n",
        "        self.build_index(self.documents)\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 5, alpha: float = 0.7) -> Tuple[List[Document], List[float], List[float]]:\n",
        "        \"\"\"Hybrid retrieval with dense + sparse fusion\"\"\"\n",
        "        # Dense retrieval\n",
        "        q_embed = self.dense_encoder.encode([query], convert_to_numpy=True)\n",
        "        faiss.normalize_L2(q_embed)\n",
        "        dense_scores, dense_idx = self.dense_index.search(q_embed.astype(\"float32\"), k * 2)\n",
        "        dense_scores, dense_idx = dense_scores[0].tolist(), dense_idx[0].tolist()\n",
        "\n",
        "        # Sparse retrieval\n",
        "        query_tokens = query.lower().split()\n",
        "        sparse_scores = self.bm25.get_scores(query_tokens)\n",
        "        sparse_top_idx = np.argsort(sparse_scores)[-k * 2:][::-1].tolist()\n",
        "\n",
        "        # Fusion\n",
        "        max_dense = max(dense_scores) if dense_scores else 1.0\n",
        "        max_sparse = max(sparse_scores) if max(sparse_scores) > 0 else 1.0\n",
        "\n",
        "        doc_scores = {}\n",
        "        for idx, score in zip(dense_idx, dense_scores):\n",
        "            doc_scores[idx] = alpha * (score / max_dense)\n",
        "\n",
        "        for idx in sparse_top_idx:\n",
        "            current = doc_scores.get(idx, 0.0)\n",
        "            doc_scores[idx] = current + (1 - alpha) * (sparse_scores[idx] / max_sparse)\n",
        "\n",
        "        # Top-k fusion scores\n",
        "        top_fused = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "        candidate_docs = [self.documents[i] for i, _ in top_fused]\n",
        "        candidate_scores = [s for _, s in top_fused]\n",
        "\n",
        "        # Reranking\n",
        "        pairs = [[query, f\"{d.title}: {d.content}\"] for d in candidate_docs]\n",
        "        rerank_scores = self.reranker.predict(pairs, show_progress_bar=False)\n",
        "\n",
        "        # Sort by rerank scores\n",
        "        sorted_indices = np.argsort(rerank_scores)[::-1]\n",
        "\n",
        "        final_docs = [candidate_docs[i] for i in sorted_indices]\n",
        "        final_hybrid_scores = [candidate_scores[i] for i in sorted_indices]\n",
        "        final_rerank_scores = [float(rerank_scores[i]) for i in sorted_indices]\n",
        "\n",
        "        return final_docs, final_hybrid_scores, final_rerank_scores\n",
        "\n",
        "    def calculate_retrieval_recall(self, retrieved_docs: List[Document],\n",
        "                                   supporting_facts: List[List]) -> float:\n",
        "        \"\"\"Calculate recall@k for retrieved documents\"\"\"\n",
        "        if not supporting_facts:\n",
        "            return 0.0\n",
        "\n",
        "        retrieved_titles = set(d.title for d in retrieved_docs)\n",
        "        gold_titles = set(title for title, _ in supporting_facts)\n",
        "\n",
        "        if not gold_titles:\n",
        "            return 0.0\n",
        "\n",
        "        overlap = retrieved_titles & gold_titles\n",
        "        return len(overlap) / len(gold_titles)"
      ],
      "metadata": {
        "id": "0wOhrkDpsDoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Enhanced Extractive QA with Fine-tuning\n",
        "# ======================================================================\n",
        "\n",
        "class EnhancedExtractiveQA:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.use_gpu = config.use_gpu and torch.cuda.is_available()\n",
        "        self.device = \"cuda\" if self.use_gpu else \"cpu\"\n",
        "\n",
        "        logger.info(f\"Loading extractive QA model on {self.device}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.extractive_model)\n",
        "        self.model = AutoModelForQuestionAnswering.from_pretrained(config.extractive_model).to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def prepare_training_data(self, examples: List[Dict], documents: Dict[str, Document]) -> Dataset:\n",
        "        \"\"\"Prepare training data for extractive QA\"\"\"\n",
        "        logger.info(\"Preparing extractive QA training data...\")\n",
        "\n",
        "        training_samples = []\n",
        "\n",
        "        for ex in tqdm(examples, desc=\"Creating QA pairs\"):\n",
        "            question = ex[\"question\"]\n",
        "            answer = ex[\"answer\"]\n",
        "\n",
        "            # Get supporting documents\n",
        "            supporting_titles = set(title for title, _ in ex[\"supporting_facts\"])\n",
        "\n",
        "            for title in supporting_titles:\n",
        "                if title in documents:\n",
        "                    doc = documents[title]\n",
        "                    context = doc.content\n",
        "\n",
        "                    # Find answer in context\n",
        "                    answer_start = context.lower().find(answer.lower())\n",
        "\n",
        "                    if answer_start != -1:\n",
        "                        training_samples.append({\n",
        "                            \"question\": question,\n",
        "                            \"context\": context,\n",
        "                            \"answer_start\": answer_start,\n",
        "                            \"answer_text\": answer\n",
        "                        })\n",
        "\n",
        "        logger.info(f\"Created {len(training_samples)} training samples\")\n",
        "        return Dataset.from_list(training_samples)\n",
        "\n",
        "    def preprocess_function(self, examples):\n",
        "        \"\"\"Tokenize and prepare inputs\"\"\"\n",
        "        questions = examples[\"question\"]\n",
        "        contexts = examples[\"context\"]\n",
        "\n",
        "        tokenized = self.tokenizer(\n",
        "            questions,\n",
        "            contexts,\n",
        "            max_length=self.config.max_length,\n",
        "            truncation=\"only_second\",\n",
        "            padding=\"max_length\",\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        offset_mapping = tokenized.pop(\"offset_mapping\")\n",
        "        start_positions = []\n",
        "        end_positions = []\n",
        "\n",
        "        for i, (answer_start, answer_text) in enumerate(zip(examples[\"answer_start\"], examples[\"answer_text\"])):\n",
        "            answer_end = answer_start + len(answer_text)\n",
        "\n",
        "            # Find token positions\n",
        "            sequence_ids = tokenized.sequence_ids(i)\n",
        "            context_start = 0\n",
        "            while sequence_ids[context_start] != 1:\n",
        "                context_start += 1\n",
        "            context_end = len(sequence_ids) - 1\n",
        "            while sequence_ids[context_end] != 1:\n",
        "                context_end -= 1\n",
        "\n",
        "            # Find start and end token positions\n",
        "            token_start = context_start\n",
        "            while token_start <= context_end and offset_mapping[i][token_start][0] <= answer_start:\n",
        "                token_start += 1\n",
        "            start_positions.append(token_start - 1)\n",
        "\n",
        "            token_end = context_end\n",
        "            while token_end >= context_start and offset_mapping[i][token_end][1] >= answer_end:\n",
        "                token_end -= 1\n",
        "            end_positions.append(token_end + 1)\n",
        "\n",
        "        tokenized[\"start_positions\"] = start_positions\n",
        "        tokenized[\"end_positions\"] = end_positions\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def fine_tune(self, train_examples: List[Dict], val_examples: List[Dict], doc_map: Dict[str, Document]):\n",
        "        \"\"\"Fine-tune extractive QA model\"\"\"\n",
        "        logger.info(\"=\"*60)\n",
        "        logger.info(\"FINE-TUNING EXTRACTIVE QA MODEL\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        # Prepare datasets\n",
        "        train_dataset = self.prepare_training_data(train_examples, doc_map)\n",
        "\n",
        "        if not train_dataset:\n",
        "            logger.warning(\"No training data available, skipping fine-tuning\")\n",
        "            return\n",
        "\n",
        "        # Tokenize\n",
        "        tokenized_train = train_dataset.map(\n",
        "            self.preprocess_function,\n",
        "            batched=True,\n",
        "            remove_columns=train_dataset.column_names\n",
        "        )\n",
        "\n",
        "        # Training arguments\n",
        "        output_dir = Path(self.config.checkpoint_dir) / \"extractive_qa\"\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=str(output_dir),\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            num_train_epochs=self.config.num_epochs,\n",
        "            warmup_steps=self.config.warmup_steps,\n",
        "            logging_steps=100,\n",
        "            save_strategy=\"epoch\",\n",
        "            fp16=self.use_gpu,\n",
        "            report_to=\"none\",\n",
        "            use_cpu=(not self.use_gpu)\n",
        "        )\n",
        "\n",
        "        # Trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_train,\n",
        "            data_collator=DefaultDataCollator(),\n",
        "            tokenizer=self.tokenizer\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        trainer.train()\n",
        "\n",
        "        # Save\n",
        "        if self.config.save_checkpoints:\n",
        "            trainer.save_model(str(output_dir / \"final\"))\n",
        "            logger.info(f\"✓ Model saved to {output_dir / 'final'}\")\n",
        "\n",
        "        logger.info(\"✓ Extractive QA fine-tuning complete\")\n",
        "\n",
        "    def extract_answer(self, question: str, context: str) -> Tuple[str, float]:\n",
        "        \"\"\"Extract answer span from context\"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            question, context,\n",
        "            max_length=self.config.max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            start_logits = outputs.start_logits[0]\n",
        "            end_logits = outputs.end_logits[0]\n",
        "\n",
        "        start_idx = torch.argmax(start_logits).item()\n",
        "        end_idx = torch.argmax(end_logits).item()\n",
        "\n",
        "        if end_idx < start_idx or (end_idx - start_idx) > 30:\n",
        "            return \"\", 0.0\n",
        "\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        answer = self.tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+1])\n",
        "\n",
        "        confidence = (start_logits[start_idx] + end_logits[end_idx]).item() / 2\n",
        "        return answer.strip(), float(torch.sigmoid(torch.tensor(confidence)).item())\n"
      ],
      "metadata": {
        "id": "AOdO_VArsDtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Enhanced Generative QA\n",
        "# ======================================================================\n",
        "\n",
        "class EnhancedGenerativeQA:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.use_gpu = config.use_gpu and torch.cuda.is_available()\n",
        "\n",
        "        logger.info(\"Loading generative model...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.generative_model, trust_remote_code=True)\n",
        "\n",
        "        if self.use_gpu and config.use_4bit_quant:\n",
        "            quant_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\"\n",
        "            )\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                config.generative_model,\n",
        "                quantization_config=quant_config,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=torch.float16\n",
        "            )\n",
        "        else:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                config.generative_model,\n",
        "                device_map=\"auto\" if self.use_gpu else None,\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=torch.float16 if self.use_gpu else torch.float32\n",
        "            )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate_answer(self, question: str, context: str) -> Tuple[str, float]:\n",
        "        \"\"\"Generate answer from context\"\"\"\n",
        "        prompt = f\"\"\"Based on the context, answer the question concisely.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=1536,\n",
        "            truncation=True\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                top_p=0.95,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Calculate confidence\n",
        "        scores = torch.stack(outputs.scores)\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "        max_probs = probs.max(dim=-1).values\n",
        "        confidence = max_probs.mean().item()\n",
        "\n",
        "        answer = self.tokenizer.decode(\n",
        "            outputs.sequences[0][len(inputs[\"input_ids\"][0]):],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "\n",
        "        # Clean answer\n",
        "        for prefix in [\"Answer:\", \"A:\", \"The answer is\", \"answer:\"]:\n",
        "            if answer.lower().startswith(prefix.lower()):\n",
        "                answer = answer[len(prefix):].strip()\n",
        "\n",
        "        # Reject invalid answers\n",
        "        invalid = [\"unknown\", \"i don't know\", \"cannot determine\", \"not enough information\", \"unclear\"]\n",
        "        if any(inv in answer.lower() for inv in invalid) or len(answer.split()) < 2:\n",
        "            return \"\", 0.0\n",
        "\n",
        "        return answer, confidence\n"
      ],
      "metadata": {
        "id": "mKZ0p8qhsDzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Full Hybrid QA System\n",
        "# ======================================================================\n",
        "\n",
        "class FullHybridQASystem:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        logger.info(\"Initializing Full Hybrid QA System...\")\n",
        "\n",
        "        self.extractive = EnhancedExtractiveQA(config)\n",
        "        self.generative = EnhancedGenerativeQA(config)\n",
        "        self.verifier = CrossEncoder(\n",
        "            config.rerank_model,\n",
        "            device=\"cuda\" if config.use_gpu and torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "\n",
        "    def _is_multi_hop(self, question: str) -> bool:\n",
        "        \"\"\"Detect multi-hop questions\"\"\"\n",
        "        multi_hop_patterns = [\n",
        "            r'\\b(who|what|when|where|which)\\b.*\\b(who|what|when|where|which)\\b',\n",
        "            r'\\b(and|also|both|between)\\b',\n",
        "            r'\\b(compare|contrast|relate)\\b'\n",
        "        ]\n",
        "        q_lower = question.lower()\n",
        "        return any(re.search(pattern, q_lower) for pattern in multi_hop_patterns)\n",
        "\n",
        "    def answer(self, question: str, docs: List[Document]) -> Tuple[str, str, float, int, int, int]:\n",
        "        \"\"\"Generate answer using hybrid approach\"\"\"\n",
        "        # Combine contexts\n",
        "        context = \"\\n\\n\".join([f\"{d.title}: {d.content[:800]}\" for d in docs[:5]])\n",
        "\n",
        "        # Try both\n",
        "        ext_answer, ext_conf = self.extractive.extract_answer(question, context)\n",
        "        gen_answer, gen_conf = self.generative.generate_answer(question, context)\n",
        "\n",
        "        # Verify\n",
        "        ext_verify = self.verifier.predict([[question, ext_answer]])[0] if ext_answer else 0.0\n",
        "        gen_verify = self.verifier.predict([[question, gen_answer]])[0] if gen_answer else 0.0\n",
        "\n",
        "        # Route decision\n",
        "        is_multi_hop = self._is_multi_hop(question)\n",
        "\n",
        "        if is_multi_hop:\n",
        "            if gen_answer and gen_verify > 0.5:\n",
        "                answer, ans_type, conf = gen_answer, \"generative\", gen_conf\n",
        "            elif ext_answer:\n",
        "                answer, ans_type, conf = ext_answer, \"extractive\", ext_conf\n",
        "            else:\n",
        "                answer, ans_type, conf = \"\", \"none\", 0.0\n",
        "        else:\n",
        "            if ext_answer and ext_verify > gen_verify:\n",
        "                answer, ans_type, conf = ext_answer, \"extractive\", ext_conf\n",
        "            elif gen_answer:\n",
        "                answer, ans_type, conf = gen_answer, \"generative\", gen_conf\n",
        "            elif ext_answer:\n",
        "                answer, ans_type, conf = ext_answer, \"extractive\", ext_conf\n",
        "            else:\n",
        "                answer, ans_type, conf = \"\", \"none\", 0.0\n",
        "\n",
        "        # Token counts\n",
        "        p_tok = len(self.generative.tokenizer.encode(question + context[:500]))\n",
        "        o_tok = len(self.generative.tokenizer.encode(answer)) if answer else 0\n",
        "\n",
        "        return answer, ans_type, conf, p_tok, o_tok, p_tok + o_tok\n"
      ],
      "metadata": {
        "id": "WXY7JJxLsD4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Full Pipeline\n",
        "# ======================================================================\n",
        "\n",
        "class FullHybridRAGPipeline:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        logger.info(\"=\"*60)\n",
        "        logger.info(\"INITIALIZING FULL-SCALE HYBRID RAG PIPELINE\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        self.retriever = EnhancedHybridRetriever(config)\n",
        "        self.qa_system = FullHybridQASystem(config)\n",
        "        self.all_documents = []\n",
        "        self.doc_map = {}\n",
        "\n",
        "    def load_and_process_data(self):\n",
        "        \"\"\"Load full HotpotQA dataset\"\"\"\n",
        "        logger.info(\"Loading HotpotQA dataset...\")\n",
        "\n",
        "        data_path = Path(self.config.data_dir) / \"hotpot_dev_distractor_v1.json\"\n",
        "\n",
        "        if not data_path.exists():\n",
        "            logger.info(\"Downloading HotpotQA...\")\n",
        "            from datasets import load_dataset\n",
        "\n",
        "            # Load full dataset\n",
        "            train_dset = load_dataset(\"hotpot_qa\", \"distractor\", split=\"train\", trust_remote_code=True)\n",
        "            val_dset = load_dataset(\"hotpot_qa\", \"distractor\", split=\"validation\", trust_remote_code=True)\n",
        "\n",
        "            def convert_item(item):\n",
        "                return {\n",
        "                    \"_id\": item[\"id\"],\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"answer\": item[\"answer\"],\n",
        "                    \"type\": item[\"type\"],\n",
        "                    \"level\": item[\"level\"],\n",
        "                    \"context\": [[t, s] for t, s in zip(item[\"context\"][\"title\"], item[\"context\"][\"sentences\"])],\n",
        "                    \"supporting_facts\": [[t, sid] for t, sid in zip(item[\"supporting_facts\"][\"title\"], item[\"supporting_facts\"][\"sent_id\"])]\n",
        "                }\n",
        "\n",
        "            logger.info(\"Converting training data...\")\n",
        "            train_data = [convert_item(item) for item in tqdm(train_dset)]\n",
        "\n",
        "            logger.info(\"Converting validation data...\")\n",
        "            val_data = [convert_item(item) for item in tqdm(val_dset)]\n",
        "\n",
        "            # Save\n",
        "            Path(self.config.data_dir).mkdir(exist_ok=True)\n",
        "            with open(Path(self.config.data_dir) / \"hotpot_train.json\", \"w\") as f:\n",
        "                json.dump(train_data, f, indent=2)\n",
        "            with open(data_path, \"w\") as f:\n",
        "                json.dump(val_data, f, indent=2)\n",
        "\n",
        "            logger.info(f\"✓ Saved {len(train_data)} training examples\")\n",
        "            logger.info(f\"✓ Saved {len(val_data)} validation examples\")\n",
        "\n",
        "        # Load data\n",
        "        train_path = Path(self.config.data_dir) / \"hotpot_train.json\"\n",
        "\n",
        "        with open(train_path) as f:\n",
        "            train_data = json.load(f)\n",
        "        with open(data_path) as f:\n",
        "            val_data = json.load(f)\n",
        "\n",
        "        # Limit if specified\n",
        "        if self.config.num_train_examples > 0:\n",
        "            train_data = train_data[:self.config.num_train_examples]\n",
        "        if self.config.num_val_examples > 0:\n",
        "            val_data = val_data[:self.config.num_val_examples]\n",
        "\n",
        "        logger.info(f\"✓ Loaded {len(train_data)} train, {len(val_data)} val examples\")\n",
        "\n",
        "        return train_data, val_data\n",
        "\n",
        "    def build_document_index(self, examples: List[Dict]):\n",
        "        \"\"\"Extract and index all documents\"\"\"\n",
        "        logger.info(\"Extracting documents from examples...\")\n",
        "\n",
        "        docs = []\n",
        "        for ex in tqdm(examples, desc=\"Processing\"):\n",
        "            for i, (title, sentences) in enumerate(ex[\"context\"]):\n",
        "                docs.append(Document(\n",
        "                    id=f\"{ex['_id']}_{i}\",\n",
        "                    title=title,\n",
        "                    content=\" \".join(sentences),\n",
        "                    sentences=sentences\n",
        "                ))\n",
        "\n",
        "        # Deduplicate by title\n",
        "        unique_docs = {d.title: d for d in docs}\n",
        "        self.all_documents = list(unique_docs.values())\n",
        "        self.doc_map = {d.title: d for d in self.all_documents}\n",
        "\n",
        "        logger.info(f\"✓ Extracted {len(self.all_documents)} unique documents\")\n",
        "\n",
        "        # Build index\n",
        "        self.retriever.build_index(self.all_documents)\n",
        "\n",
        "    def train(self, train_data: List[Dict], val_data: List[Dict]):\n",
        "        \"\"\"Train all components\"\"\"\n",
        "        logger.info(\"=\"*60)\n",
        "        logger.info(\"TRAINING PHASE\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        # Train retriever\n",
        "        if self.config.train_retriever:\n",
        "            self.retriever.fine_tune(train_data, val_data)\n",
        "\n",
        "        # Train extractive QA\n",
        "        if self.config.train_extractive:\n",
        "            self.qa_system.extractive.fine_tune(train_data, val_data, self.doc_map)\n",
        "\n",
        "        logger.info(\"✓ Training complete!\")\n",
        "\n",
        "    def evaluate(self, val_data: List[Dict], output_file: str):\n",
        "        \"\"\"Evaluate on validation set\"\"\"\n",
        "        logger.info(\"=\"*60)\n",
        "        logger.info(\"EVALUATION PHASE\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "            for ex in tqdm(val_data, desc=\"Evaluating\"):\n",
        "                # Retrieve\n",
        "                docs, h_scores, r_scores = self.retriever.retrieve(\n",
        "                    ex[\"question\"],\n",
        "                    k=self.config.retrieval_k,\n",
        "                    alpha=self.config.alpha\n",
        "                )\n",
        "\n",
        "                # Calculate retrieval recall\n",
        "                retrieval_recall = self.retriever.calculate_retrieval_recall(\n",
        "                    docs, ex.get(\"supporting_facts\", [])\n",
        "                )\n",
        "\n",
        "                # Answer\n",
        "                answer, ans_type, conf, p_tok, o_tok, t_tok = self.qa_system.answer(ex[\"question\"], docs)\n",
        "\n",
        "                output = RAGOutput(\n",
        "                    question_id=ex[\"_id\"],\n",
        "                    question=ex[\"question\"],\n",
        "                    answer=answer,\n",
        "                    answer_type=ans_type,\n",
        "                    gold_answer=ex[\"answer\"],\n",
        "                    retrieved_passages=[{\n",
        "                        \"passage_id\": d.id,\n",
        "                        \"title\": d.title,\n",
        "                        \"content\": d.content,\n",
        "                        \"hybrid_score\": float(hs),\n",
        "                        \"rerank_score\": float(rs)\n",
        "                    } for d, hs, rs in zip(docs, h_scores, r_scores)],\n",
        "                    confidence_score=conf,\n",
        "                    prompt_tokens=p_tok,\n",
        "                    output_tokens=o_tok,\n",
        "                    total_tokens=t_tok,\n",
        "                    retrieval_scores=h_scores,\n",
        "                    supporting_facts=ex.get(\"supporting_facts\", []),\n",
        "                    retrieval_recall=retrieval_recall\n",
        "                )\n",
        "\n",
        "                results.append(output)\n",
        "                f.write(json.dumps(asdict(output)) + \"\\n\")\n",
        "\n",
        "        logger.info(f\"✓ Saved predictions to {output_file}\")\n",
        "        return results"
      ],
      "metadata": {
        "id": "CByOELdusD-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Enhanced Evaluator\n",
        "# ======================================================================\n",
        "\n",
        "class EnhancedRAGEvaluator:\n",
        "    def normalize_answer(self, text: str) -> str:\n",
        "        \"\"\"Normalize answer text\"\"\"\n",
        "        text = text.lower().strip()\n",
        "        text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def exact_match(self, predicted: str, gold: str) -> float:\n",
        "        \"\"\"Calculate exact match\"\"\"\n",
        "        return 1.0 if self.normalize_answer(predicted) == self.normalize_answer(gold) else 0.0\n",
        "\n",
        "    def f1_score(self, predicted: str, gold: str) -> float:\n",
        "        \"\"\"Calculate token-level F1 score\"\"\"\n",
        "        pred_tokens = set(self.normalize_answer(predicted).split())\n",
        "        gold_tokens = set(self.normalize_answer(gold).split())\n",
        "\n",
        "        if not pred_tokens or not gold_tokens:\n",
        "            return 0.0\n",
        "\n",
        "        common = pred_tokens & gold_tokens\n",
        "        if not common:\n",
        "            return 0.0\n",
        "\n",
        "        precision = len(common) / len(pred_tokens)\n",
        "        recall = len(common) / len(gold_tokens)\n",
        "\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    def evaluate_results(self, results_jsonl: str, original_data: List[Dict]) -> Dict:\n",
        "        \"\"\"Comprehensive evaluation\"\"\"\n",
        "        logger.info(\"Evaluating results...\")\n",
        "\n",
        "        predictions = [json.loads(line) for line in open(results_jsonl)]\n",
        "        gold_map = {ex[\"_id\"]: ex for ex in original_data}\n",
        "\n",
        "        metrics = {\n",
        "            \"overall\": {\"em\": [], \"f1\": [], \"retrieval_recall\": []},\n",
        "            \"by_type\": {},\n",
        "            \"by_question_type\": {},\n",
        "            \"by_level\": {}\n",
        "        }\n",
        "\n",
        "        for pred in predictions:\n",
        "            qid = pred[\"question_id\"]\n",
        "            if qid not in gold_map:\n",
        "                continue\n",
        "\n",
        "            gold_ex = gold_map[qid]\n",
        "            gold_answer = gold_ex[\"answer\"]\n",
        "\n",
        "            # Calculate metrics\n",
        "            em = self.exact_match(pred[\"answer\"], gold_answer)\n",
        "            f1 = self.f1_score(pred[\"answer\"], gold_answer)\n",
        "            ret_recall = pred.get(\"retrieval_recall\", 0.0)\n",
        "\n",
        "            # Overall\n",
        "            metrics[\"overall\"][\"em\"].append(em)\n",
        "            metrics[\"overall\"][\"f1\"].append(f1)\n",
        "            metrics[\"overall\"][\"retrieval_recall\"].append(ret_recall)\n",
        "\n",
        "            # By answer type\n",
        "            ans_type = pred.get(\"answer_type\", \"unknown\")\n",
        "            if ans_type not in metrics[\"by_type\"]:\n",
        "                metrics[\"by_type\"][ans_type] = {\"em\": [], \"f1\": [], \"count\": 0}\n",
        "            metrics[\"by_type\"][ans_type][\"em\"].append(em)\n",
        "            metrics[\"by_type\"][ans_type][\"f1\"].append(f1)\n",
        "            metrics[\"by_type\"][ans_type][\"count\"] += 1\n",
        "\n",
        "            # By question type\n",
        "            q_type = gold_ex.get(\"type\", \"unknown\")\n",
        "            if q_type not in metrics[\"by_question_type\"]:\n",
        "                metrics[\"by_question_type\"][q_type] = {\"em\": [], \"f1\": []}\n",
        "            metrics[\"by_question_type\"][q_type][\"em\"].append(em)\n",
        "            metrics[\"by_question_type\"][q_type][\"f1\"].append(f1)\n",
        "\n",
        "            # By difficulty level\n",
        "            level = gold_ex.get(\"level\", \"unknown\")\n",
        "            if level not in metrics[\"by_level\"]:\n",
        "                metrics[\"by_level\"][level] = {\"em\": [], \"f1\": []}\n",
        "            metrics[\"by_level\"][level][\"em\"].append(em)\n",
        "            metrics[\"by_level\"][level][\"f1\"].append(f1)\n",
        "\n",
        "        # Aggregate\n",
        "        results = {\n",
        "            \"overall\": {\n",
        "                \"exact_match\": float(np.mean(metrics[\"overall\"][\"em\"])),\n",
        "                \"f1_score\": float(np.mean(metrics[\"overall\"][\"f1\"])),\n",
        "                \"retrieval_recall\": float(np.mean(metrics[\"overall\"][\"retrieval_recall\"])),\n",
        "                \"num_examples\": len(metrics[\"overall\"][\"em\"])\n",
        "            },\n",
        "            \"by_type\": {\n",
        "                k: {\n",
        "                    \"exact_match\": float(np.mean(v[\"em\"])),\n",
        "                    \"f1_score\": float(np.mean(v[\"f1\"])),\n",
        "                    \"count\": v[\"count\"]\n",
        "                } for k, v in metrics[\"by_type\"].items()\n",
        "            },\n",
        "            \"by_question_type\": {\n",
        "                k: {\n",
        "                    \"exact_match\": float(np.mean(v[\"em\"])),\n",
        "                    \"f1_score\": float(np.mean(v[\"f1\"])),\n",
        "                    \"count\": len(v[\"em\"])\n",
        "                } for k, v in metrics[\"by_question_type\"].items()\n",
        "            },\n",
        "            \"by_level\": {\n",
        "                k: {\n",
        "                    \"exact_match\": float(np.mean(v[\"em\"])),\n",
        "                    \"f1_score\": float(np.mean(v[\"f1\"])),\n",
        "                    \"count\": len(v[\"em\"])\n",
        "                } for k, v in metrics[\"by_level\"].items()\n",
        "            },\n",
        "            \"raw_scores\": metrics[\"overall\"]\n",
        "        }\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "-up-iz6qsEEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Enhanced Visualizer\n",
        "# ======================================================================\n",
        "\n",
        "class EnhancedRAGVisualizer:\n",
        "    def __init__(self, output_dir=\"visualizations\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        sns.set_style(\"whitegrid\")\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "    def plot_comprehensive_metrics(self, results: Dict, filename=\"full_results.png\"):\n",
        "        \"\"\"Create comprehensive visualization\"\"\"\n",
        "        fig = plt.figure(figsize=(20, 12))\n",
        "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "        # Overall metrics\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        overall = results[\"overall\"]\n",
        "        metrics = [\"EM\", \"F1\", \"Ret.\\nRecall\"]\n",
        "        values = [overall[\"exact_match\"], overall[\"f1_score\"], overall[\"retrieval_recall\"]]\n",
        "        bars = ax1.bar(metrics, values, color=[\"#2ecc71\", \"#3498db\", \"#9b59b6\"], alpha=0.8)\n",
        "        ax1.set_ylim(0, 1)\n",
        "        ax1.set_title(\"Overall Performance\", fontweight=\"bold\", fontsize=12)\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        # By answer type\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        types = list(results[\"by_type\"].keys())\n",
        "        if types:\n",
        "            x = np.arange(len(types))\n",
        "            width = 0.35\n",
        "            em_vals = [results[\"by_type\"][t][\"exact_match\"] for t in types]\n",
        "            f1_vals = [results[\"by_type\"][t][\"f1_score\"] for t in types]\n",
        "            ax2.bar(x - width/2, em_vals, width, label=\"EM\", alpha=0.8)\n",
        "            ax2.bar(x + width/2, f1_vals, width, label=\"F1\", alpha=0.8)\n",
        "            ax2.set_xticks(x)\n",
        "            ax2.set_xticklabels(types, rotation=45, ha=\"right\")\n",
        "            ax2.legend()\n",
        "            ax2.set_title(\"Performance by Answer Type\", fontweight=\"bold\", fontsize=12)\n",
        "            ax2.set_ylim(0, 1)\n",
        "\n",
        "        # By question type\n",
        "        ax3 = fig.add_subplot(gs[0, 2])\n",
        "        q_types = list(results[\"by_question_type\"].keys())\n",
        "        if q_types:\n",
        "            x = np.arange(len(q_types))\n",
        "            width = 0.35\n",
        "            em_vals = [results[\"by_question_type\"][t][\"exact_match\"] for t in q_types]\n",
        "            f1_vals = [results[\"by_question_type\"][t][\"f1_score\"] for t in q_types]\n",
        "            ax3.bar(x - width/2, em_vals, width, label=\"EM\", alpha=0.8)\n",
        "            ax3.bar(x + width/2, f1_vals, width, label=\"F1\", alpha=0.8)\n",
        "            ax3.set_xticks(x)\n",
        "            ax3.set_xticklabels(q_types, rotation=45, ha=\"right\")\n",
        "            ax3.legend()\n",
        "            ax3.set_title(\"Performance by Question Type\", fontweight=\"bold\", fontsize=12)\n",
        "            ax3.set_ylim(0, 1)\n",
        "\n",
        "        # By difficulty level\n",
        "        ax4 = fig.add_subplot(gs[1, 0])\n",
        "        levels = list(results[\"by_level\"].keys())\n",
        "        if levels:\n",
        "            x = np.arange(len(levels))\n",
        "            width = 0.35\n",
        "            em_vals = [results[\"by_level\"][lv][\"exact_match\"] for lv in levels]\n",
        "            f1_vals = [results[\"by_level\"][lv][\"f1_score\"] for lv in levels]\n",
        "            ax4.bar(x - width/2, em_vals, width, label=\"EM\", alpha=0.8)\n",
        "            ax4.bar(x + width/2, f1_vals, width, label=\"F1\", alpha=0.8)\n",
        "            ax4.set_xticks(x)\n",
        "            ax4.set_xticklabels(levels, rotation=45, ha=\"right\")\n",
        "            ax4.legend()\n",
        "            ax4.set_title(\"Performance by Difficulty\", fontweight=\"bold\", fontsize=12)\n",
        "            ax4.set_ylim(0, 1)\n",
        "\n",
        "        # F1 distribution\n",
        "        ax5 = fig.add_subplot(gs[1, 1])\n",
        "        f1_scores = results[\"raw_scores\"][\"f1\"]\n",
        "        ax5.hist(f1_scores, bins=30, color=\"#3498db\", alpha=0.7, edgecolor=\"black\")\n",
        "        ax5.axvline(np.mean(f1_scores), color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean: {np.mean(f1_scores):.3f}\")\n",
        "        ax5.set_xlabel(\"F1 Score\")\n",
        "        ax5.set_ylabel(\"Frequency\")\n",
        "        ax5.set_title(\"F1 Score Distribution\", fontweight=\"bold\", fontsize=12)\n",
        "        ax5.legend()\n",
        "\n",
        "        # EM distribution\n",
        "        ax6 = fig.add_subplot(gs[1, 2])\n",
        "        em_scores = results[\"raw_scores\"][\"em\"]\n",
        "        unique_em, counts_em = np.unique(em_scores, return_counts=True)\n",
        "        colors = [\"#e74c3c\" if val == 0 else \"#2ecc71\" for val in unique_em]\n",
        "        ax6.bar([\"Incorrect\", \"Correct\"], counts_em, color=colors, alpha=0.8, edgecolor=\"black\")\n",
        "        ax6.set_ylabel(\"Count\")\n",
        "        ax6.set_title(\"Exact Match Distribution\", fontweight=\"bold\", fontsize=12)\n",
        "        for i, (val, count) in enumerate(zip(unique_em, counts_em)):\n",
        "            ax6.text(i, count, f'{count}\\n({count/len(em_scores)*100:.1f}%)',\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        # Retrieval recall distribution\n",
        "        ax7 = fig.add_subplot(gs[2, 0])\n",
        "        ret_recalls = results[\"raw_scores\"][\"retrieval_recall\"]\n",
        "        ax7.hist(ret_recalls, bins=30, color=\"#9b59b6\", alpha=0.7, edgecolor=\"black\")\n",
        "        ax7.axvline(np.mean(ret_recalls), color=\"red\", linestyle=\"--\", linewidth=2,\n",
        "                   label=f\"Mean: {np.mean(ret_recalls):.3f}\")\n",
        "        ax7.set_xlabel(\"Retrieval Recall\")\n",
        "        ax7.set_ylabel(\"Frequency\")\n",
        "        ax7.set_title(\"Retrieval Recall Distribution\", fontweight=\"bold\", fontsize=12)\n",
        "        ax7.legend()\n",
        "\n",
        "        # Answer type distribution\n",
        "        ax8 = fig.add_subplot(gs[2, 1])\n",
        "        if results[\"by_type\"]:\n",
        "            type_names = list(results[\"by_type\"].keys())\n",
        "            type_counts = [results[\"by_type\"][t][\"count\"] for t in type_names]\n",
        "            colors_pie = plt.cm.Set3(range(len(type_names)))\n",
        "            ax8.pie(type_counts, labels=type_names, autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
        "            ax8.set_title(\"Answer Type Distribution\", fontweight=\"bold\", fontsize=12)\n",
        "\n",
        "        # Performance summary table\n",
        "        ax9 = fig.add_subplot(gs[2, 2])\n",
        "        ax9.axis('tight')\n",
        "        ax9.axis('off')\n",
        "\n",
        "        table_data = [\n",
        "            [\"Metric\", \"Value\"],\n",
        "            [\"Overall EM\", f\"{overall['exact_match']:.3f}\"],\n",
        "            [\"Overall F1\", f\"{overall['f1_score']:.3f}\"],\n",
        "            [\"Retrieval Recall\", f\"{overall['retrieval_recall']:.3f}\"],\n",
        "            [\"Total Examples\", f\"{overall['num_examples']}\"]\n",
        "        ]\n",
        "\n",
        "        table = ax9.table(cellText=table_data, cellLoc='left', loc='center',\n",
        "                         colWidths=[0.5, 0.5])\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(10)\n",
        "        table.scale(1, 2)\n",
        "\n",
        "        # Style header row\n",
        "        for i in range(2):\n",
        "            table[(0, i)].set_facecolor('#3498db')\n",
        "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "        ax9.set_title(\"Performance Summary\", fontweight=\"bold\", fontsize=12, pad=20)\n",
        "\n",
        "        plt.suptitle(\"Full-Scale Hybrid RAG Performance Analysis\",\n",
        "                    fontsize=16, fontweight=\"bold\", y=0.98)\n",
        "\n",
        "        save_path = self.output_dir / filename\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "        logger.info(f\"✓ Saved visualization: {save_path}\")\n",
        "\n",
        "    def plot_training_progress(self, train_metrics: Dict, filename=\"training_progress.png\"):\n",
        "        \"\"\"Plot training progress if available\"\"\"\n",
        "        # Placeholder for training metrics visualization\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "sIkeqj9bsebx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Main Execution\n",
        "# ======================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" \"*15 + \"FULL-SCALE HYBRID RAG PIPELINE\")\n",
        "    print(\" \"*10 + \"Extractive + Generative with Fine-tuning\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Configuration\n",
        "    config = RAGConfig(\n",
        "        use_full_dataset=True,\n",
        "        train_retriever=True,\n",
        "        train_extractive=True,\n",
        "        num_train_examples=1000,  # Use 10K for training (adjust based on resources)\n",
        "        num_val_examples=100,     # Use 1K for validation\n",
        "        use_gpu=torch.cuda.is_available(),\n",
        "        save_checkpoints=True\n",
        "    )\n",
        "\n",
        "    # Display configuration\n",
        "    print(\"Configuration:\")\n",
        "    print(f\"  GPU Available: {config.use_gpu}\")\n",
        "    if config.use_gpu:\n",
        "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Train Examples: {config.num_train_examples if config.num_train_examples > 0 else 'ALL'}\")\n",
        "    print(f\"  Val Examples: {config.num_val_examples if config.num_val_examples > 0 else 'ALL'}\")\n",
        "    print(f\"  Train Retriever: {config.train_retriever}\")\n",
        "    print(f\"  Train Extractive QA: {config.train_extractive}\")\n",
        "    print()\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipeline = FullHybridRAGPipeline(config)\n",
        "\n",
        "    # Load data\n",
        "    train_data, val_data = pipeline.load_and_process_data()\n",
        "\n",
        "    # Build document index\n",
        "    all_examples = train_data + val_data\n",
        "    pipeline.build_document_index(all_examples)\n",
        "\n",
        "    # Training phase\n",
        "    if config.train_retriever or config.train_extractive:\n",
        "        pipeline.train(train_data, val_data)\n",
        "    else:\n",
        "        logger.info(\"Skipping training (training disabled in config)\")\n",
        "\n",
        "    # Evaluation phase\n",
        "    output_file = Path(config.output_dir) / \"full_predictions.jsonl\"\n",
        "    results = pipeline.evaluate(val_data, str(output_file))\n",
        "\n",
        "    # Evaluate metrics\n",
        "    evaluator = EnhancedRAGEvaluator()\n",
        "    metrics = evaluator.evaluate_results(str(output_file), val_data)\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_file = Path(config.output_dir) / \"evaluation_metrics.json\"\n",
        "    with open(metrics_file, \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    logger.info(f\"✓ Saved metrics to {metrics_file}\")\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" \"*25 + \"FINAL RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nOverall Performance:\")\n",
        "    print(f\"  Exact Match (EM):     {metrics['overall']['exact_match']:.4f}\")\n",
        "    print(f\"  F1 Score:             {metrics['overall']['f1_score']:.4f}\")\n",
        "    print(f\"  Retrieval Recall:     {metrics['overall']['retrieval_recall']:.4f}\")\n",
        "    print(f\"  Total Examples:       {metrics['overall']['num_examples']}\")\n",
        "\n",
        "    print(f\"\\nPerformance by Answer Type:\")\n",
        "    for ans_type, scores in metrics[\"by_type\"].items():\n",
        "        print(f\"  {ans_type.capitalize():15} - EM: {scores['exact_match']:.4f}, \"\n",
        "              f\"F1: {scores['f1_score']:.4f}, Count: {scores['count']}\")\n",
        "\n",
        "    print(f\"\\nPerformance by Question Type:\")\n",
        "    for q_type, scores in metrics[\"by_question_type\"].items():\n",
        "        print(f\"  {q_type.capitalize():15} - EM: {scores['exact_match']:.4f}, \"\n",
        "              f\"F1: {scores['f1_score']:.4f}, Count: {scores['count']}\")\n",
        "\n",
        "    print(f\"\\nPerformance by Difficulty Level:\")\n",
        "    for level, scores in metrics[\"by_level\"].items():\n",
        "        print(f\"  {level.capitalize():15} - EM: {scores['exact_match']:.4f}, \"\n",
        "              f\"F1: {scores['f1_score']:.4f}, Count: {scores['count']}\")\n",
        "\n",
        "    # Visualize\n",
        "    visualizer = EnhancedRAGVisualizer(output_dir=config.output_dir)\n",
        "    visualizer.plot_comprehensive_metrics(metrics)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" \"*20 + \"✓ PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nOutputs saved to: {config.output_dir}/\")\n",
        "    print(f\"  - Predictions: {output_file}\")\n",
        "    print(f\"  - Metrics: {metrics_file}\")\n",
        "    print(f\"  - Visualizations: {config.output_dir}/visualizations/\")\n",
        "    if config.save_checkpoints:\n",
        "        print(f\"  - Checkpoints: {config.checkpoint_dir}/\")\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1711e5684ff44de2b87388f013fa7d5e",
            "8900e7ed418e464696c741962e90158c",
            "b98ee10213514b9990ff7e6c01cd38e0",
            "4f1ce9f285144946b5f1fb1ac12a5043",
            "89ae1debdff54c16aabe7e136268d930",
            "af4511a066854c7f9ceffe3a4a68d62a",
            "0581bd336ff3403e88c3ae49df6c7057",
            "3be99ee95ee84679809d27e350b77802",
            "3262d0d727754ba5aa799edbdbf9be01",
            "d26319542ce44b429e5c5dd5168d774b",
            "c6e3ae5895474e7780a6b9714199ca11",
            "4b3dff4a66f349149beec060272f9e2e",
            "bf71dd603b8948c98ed6071d00047491",
            "270c960007a642d291006560d48249b2",
            "a053326b78bb4455aa389562cd2396fa",
            "21863fb0470a400cbeaf56859bbd8c48",
            "90e69acfe6244661ad200e9927a26ff3",
            "ca4099d336b8488986b7c3fe74295e82",
            "451cdcd9574a4897aca7574c34798758",
            "2eda7aac026c40858388542ae41920d3",
            "287fbe39c0d3483ba6de9f7b324bea40",
            "82d60a6e4c6649e6925f0ae02c6179e1",
            "07f587ac987c4a58907ae9bd3b14cabf",
            "62a94e70ebfc43d2abac28781ef22640",
            "5f5ca6489a214552be2e2d2a8e627d0a",
            "ec6389b3d0f543b1948eaeb8c9fc27a4",
            "751f8ecadc9240d4b3ac5e1001d4acc1",
            "ff507a9535ab4d659daaabb7448c557a",
            "6311638497254375a9e95b4004739c2a",
            "1ca19c259b0241c2ba563db79b44fc3b",
            "c504df17719b4fc9812c747da96cd089",
            "fa1f9fd59e2642bd9379671fce8805e7",
            "c73b316b14964f15b4a94651922e4d44",
            "dfac69a4a9984debb872c4d776a6a970",
            "4d5f24a9b0be42f3a01e650ad40f8fe1",
            "19bfa3bf2002429c9bc2d70b3f4c7e18",
            "921bb8dda610483fbb916c142f1a771d",
            "d5797639d66841e79e021b3f86f0718f",
            "23bf301c005c4d7bafb7407bcac22773",
            "5001f9d0981d43ceb06ea44999e8acb9",
            "88f10305fb184248bcfb4412bd0287e0",
            "565c88e1a3f04aa79eb084491e870d60",
            "9b01ba22607946b2aaba906b59937fc4",
            "368e133dee4c42f5834113fdce407d44",
            "98ab78b6b1ac478c9931c6e317c7f615",
            "5204afe18c314fd69d2529ad65bd8193",
            "84a0e68a8d7847ee85118e8419b8739c",
            "d77390ce261c4be5a6975007ba3ce1df",
            "e9028aa06f4f47c89080798d206697a4",
            "9e7b0b7c1d714f2394f18576cb0a3533",
            "58f2057182a24c548bb08e0c9f6151ee",
            "030257456ae44056b540f78040f263b0",
            "cf320eedbaad4a10bf7468b64849a984",
            "1201e082080f4fdcbb53d5154932282a",
            "e7d0119839754c278cabe28b48caa69c"
          ]
        },
        "id": "f9hTYZ4ACa3-",
        "outputId": "bbcd318c-e6aa-451b-914c-cf630b60feed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "               FULL-SCALE HYBRID RAG PIPELINE\n",
            "          Extractive + Generative with Fine-tuning\n",
            "======================================================================\n",
            "\n",
            "Configuration:\n",
            "  GPU Available: True\n",
            "  GPU: Tesla T4\n",
            "  Train Examples: 1000\n",
            "  Val Examples: 100\n",
            "  Train Retriever: True\n",
            "  Train Extractive QA: True\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1711e5684ff44de2b87388f013fa7d5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1100/1100 [00:00<00:00, 46947.66it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/336 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b3dff4a66f349149beec060272f9e2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating training pairs: 100%|██████████| 1000/1000 [00:02<00:00, 334.05it/s]\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07f587ac987c4a58907ae9bd3b14cabf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 04:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.071200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.041400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.022800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/336 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfac69a4a9984debb872c4d776a6a970"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating QA pairs: 100%|██████████| 1000/1000 [00:00<00:00, 104437.24it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1254 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98ab78b6b1ac478c9931c6e317c7f615"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [471/471 06:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.503500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.910700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.763500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.565000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 100/100 [10:02<00:00,  6.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "                         FINAL RESULTS\n",
            "======================================================================\n",
            "\n",
            "Overall Performance:\n",
            "  Exact Match (EM):     0.0800\n",
            "  F1 Score:             0.1447\n",
            "  Retrieval Recall:     0.6400\n",
            "  Total Examples:       100\n",
            "\n",
            "Performance by Answer Type:\n",
            "  Generative      - EM: 0.0247, F1: 0.0947, Count: 81\n",
            "  Extractive      - EM: 0.3333, F1: 0.3778, Count: 18\n",
            "  None            - EM: 0.0000, F1: 0.0000, Count: 1\n",
            "\n",
            "Performance by Question Type:\n",
            "  Comparison      - EM: 0.0000, F1: 0.0383, Count: 21\n",
            "  Bridge          - EM: 0.1013, F1: 0.1730, Count: 79\n",
            "\n",
            "Performance by Difficulty Level:\n",
            "  Hard            - EM: 0.0800, F1: 0.1447, Count: 100\n",
            "\n",
            "======================================================================\n",
            "                    ✓ PIPELINE COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Outputs saved to: outputs/\n",
            "  - Predictions: outputs/full_predictions.jsonl\n",
            "  - Metrics: outputs/evaluation_metrics.json\n",
            "  - Visualizations: outputs/visualizations/\n",
            "  - Checkpoints: checkpoints/\n",
            "\n"
          ]
        }
      ]
    }
  ]
}